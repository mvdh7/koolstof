{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"koolstof","text":""},{"location":"#install","title":"Install","text":"<pre><code>pip install koolstof\nconda install conda-forge::koolstof\n</code></pre>"},{"location":"#import","title":"Import","text":"<pre><code>import koolstof as ks\n</code></pre>"},{"location":"#find-current-version","title":"Find current version","text":"<pre><code>ks.hello()\n</code></pre>"},{"location":"#citation","title":"Citation","text":"<p>Humphreys, M. P., Delaigue, L. and Ourradi, Y. (2024).  Miscellaneous Python tools for marine carbonate chemistry: koolstof.  Zenodo.  doi:10.5281/zenodo.3999292.</p>"},{"location":"infrared/","title":"Infrared DIC analysis","text":"<p><code>koolstof.infrared</code> contains tools for processing infrared absorbance data to determine total dissolved inorganic carbon.</p> <p>These tools were specifically designed for infrared absorbance data recorded by a LI-COR LI-7000 via its Windows Interface Software v2.0.3 and connected to a Marianda AIRICA.</p> <p>The examples below all assume the following import convention has been used:</p> <pre><code>from koolstof import infrared as ksi\n</code></pre>"},{"location":"infrared/#data-import","title":"Data import","text":"<p><code>koolstof.infrared.io</code> contains functions to import relevant data.</p> <p>Under the hood</p> <p><code>koolstof.infrared.io.read_dbs</code> and <code>koolstof.infrared.io.read_LI7000</code> are both very light wrappers around <code>pandas.read_table</code>.  Any additional <code>kwargs</code> used with either will be passed on to that pandas function, but none should be required.</p>"},{"location":"infrared/#airica-databases","title":"AIRICA databases","text":"<p><code>koolstof.infrared.io.read_dbs</code> imports the databases generated by a Marianda AIRICA as a pandas DataFrame:</p> <pre><code>dbs = ksi.read_dbs(filename)\n</code></pre> <p>A new <code>\"datetime\"</code> column is added containing the analysis date and time as a pandas Timestamp.  It is also calculated as a Matplotlib date number in the column <code>\"datenum\"</code>.</p> <p>Finally, the original dbs column names are mapped to be more friendly.</p> dbs column name changes <p><code>koolstof.infrared.io.read_dbs</code> renames the dbs columns as follows:</p> <ul> <li><code>\"run type\"</code> \u2192 <code>\"run_type\"</code></li> <li><code>\"i.s. temp.\"</code> \u2192 <code>\"temperature_insitu\"</code></li> <li><code>\"sample mass\"</code> \u2192 <code>\"mass_sample\"</code></li> <li><code>\"rep#\"</code> \u2192 <code>\"rep\"</code></li> <li><code>\"CT\"</code> \u2192 <code>\"dic\"</code></li> <li><code>\"factor CT\"</code> \u2192 <code>\"dic_factor\"</code></li> <li><code>\"CV (\u00b5mol)\"</code> \u2192 <code>\"cv_micromol\"</code></li> <li><code>\"CV (%)\"</code> \u2192 <code>\"cv_percent\"</code></li> <li><code>\"last CRM CT\"</code> \u2192 <code>\"lastcrm_dic_measured\"</code></li> <li><code>\"cert. CRM CT\"</code> \u2192 <code>\"lastcrm_dic_certified\"</code></li> <li><code>\"CRM batch\"</code> \u2192 <code>\"lastcrm_batch\"</code></li> <li><code>\"calc. mode\"</code> \u2192 <code>\"mode_calculation\"</code></li> <li><code>\"integ. mode\"</code> \u2192 <code>\"mode_integration\"</code></li> <li><code>\"Lat.\"</code> \u2192 <code>\"latitude\"</code></li> <li><code>\"Long.\"</code> \u2192 <code>\"longitude\"</code></li> <li><code>\"area#1\"</code> \u2192 <code>\"area_1\"</code></li> <li><code>\"area#2\"</code> \u2192 <code>\"area_2\"</code></li> <li><code>\"area#3\"</code> \u2192 <code>\"area_3\"</code></li> <li><code>\"area#4\"</code> \u2192 <code>\"area_4\"</code></li> </ul> <p>All other column names are left unchanged.</p>"},{"location":"infrared/#li-7000-measurements","title":"LI-7000 measurements","text":"<p><code>koolstof.infrared.io.read_LI7000</code> imports the text files recorded by a LI-COR LI-7000 via its Windows Interface Software v2.0.3 as a pandas DataFrame:</p> <pre><code>licor = ksi.read_LI7000(filename)\n</code></pre> <p>At least the <code>\"Time\"</code> column must be present in the text file.  This is converted into a pandas Timestamp and the column is renamed as <code>\"datetime\"</code>.  It is also calculated as a Matplotlib date number in the column <code>\"datenum\"</code>.</p> <p>Later processing functions here also expect the <code>\"CO2B um/m\"</code>, <code>\"H2OB mm/m\"</code>, <code>\"T C\"</code> and <code>\"P kPa\"</code> columns.</p> <p>The original column names are mapped to be more friendly.</p> LI-7000 column name changes <p><code>koolstof.infrared.io.read_LI7000</code> renames the text file's columns as follows:</p> <ul> <li><code>\"Time\"</code> \u2192 <code>\"datetime\"</code>,</li> <li><code>\"CO2B um/m\"</code> \u2192 <code>\"x_CO2\"</code>,</li> <li><code>\"H2OB mm/m\"</code> \u2192 <code>\"x_H2O\"</code>,</li> <li><code>\"T C\"</code> \u2192 <code>\"temperature\"</code>,</li> <li><code>\"P kPa\"</code> \u2192 <code>\"pressure\"</code>,</li> <li><code>\"RH %</code>\" \u2192 <code>\"humidity_relative\"</code>,</li> <li><code>\"Flow V\"</code> \u2192 <code>\"flow_voltage\"</code>,</li> </ul> <p>Any other column names are left unchanged.</p>"},{"location":"maps/","title":"Mapping tools: <code>ks.maps</code>","text":""},{"location":"maps/#find-the-distance-to-shore","title":"Find the distance to shore","text":"<p>To calculate the distance between a point in space and the nearest shoreline, we can construct a vantage-point tree (VPT) from shoreline coordinates and then query it to find the closest point in space.</p> <p>Constructing the VPT can take a while so we can select define ranges of latitude and longitude values and only sections of coastline that intersect these ranges are included in the analysis.  The shoreline data are downloaded automatically from Natural Earth Data.  Once a VPT has been constructed it can be <code>pickle</code>d for faster future use.</p>"},{"location":"maps/#construct-the-vpt","title":"Construct the VPT","text":"<pre><code>import koolstof as ks\n\nlat_range = (50, 60)\nlon_range = (0, 10)\nvpt = ks.maps.build_vptree(lat_range, lon_range, resolution=\"10m\")\n</code></pre> <p>The <code>resolution</code> controls which scale of shoreline data is used: it can be <code>\"10m\"</code> (most detailed), <code>\"50m\"</code> or <code>\"110m\"</code>.</p>"},{"location":"maps/#use-the-vpt","title":"Use the VPT","text":"<p>The VPT can only be used to compute the distance for one point at a time.</p> <pre><code>lon = 55\nlat = 5\ndistance, nearest_lon_lat = vpt.get_nearest_neighbor((lon, lat))\n</code></pre> <p><code>distance</code> contains the distance in km to the closest point on the shoreline to <code>(lon, lat)</code>.  The coordinates of the closest point itself are given in <code>nearest_lon_lat</code>.</p>"},{"location":"spectro/","title":"Spectrophotometric pH: <code>ks.spectro</code>","text":"<p>To calculate pH on the total scale from NIOZ spectrophotometer data:</p> <pre><code>import koolstof as ks\n\npH_total = ks.spectro.pH_NIOZ(\n    absorbance_578nm,\n    absorbance_434nm,\n    absorbance_730nm,\n    temperature=25,\n    salinity=35,\n)\n</code></pre>"},{"location":"vindta/","title":"VINDTA DIC processing","text":"<p>koolstof only deals with dissolved inorganic carbon (DIC) measurements.  For total alkalinity, use Calkulate.</p> <p>koolstof provides a set of functions to import VINDTA data files as pandas DataFrames and then perform in-place operations on them to process and calibrate the data, plus some plotting functions.  There are two main processing steps: (1) applying the blank correction, and (2) calibrating to CRMs.</p> <p>Breaking changes in v0.25</p> <p>The way in which VINDTA processing works has been significantly overhauled as of koolstof v0.25.  In particular, the <code>Dbs</code> class no longer exists, so the functions described here can no longer be called as methods from the imported dbs file. In general, the change will look something like this:</p> <pre><code>dbs.do_something()     # old version, pre-v0.25\nksv.do_something(dbs)  # new version, v0.25 onwards\n</code></pre> <p>But some functions have been completely renamed and their inputs/outputs changed.  Update with caution and read the new instructions below carefully!</p> <p>All the examples here assume the following import convention:</p> <pre><code>from koolstof import vindta as ksv\n</code></pre>"},{"location":"vindta/#import-vindta-files","title":"Import VINDTA files","text":""},{"location":"vindta/#the-logfilebak","title":"The logfile.bak","text":"<p>Import a <code>logfile.bak</code> into a standard pandas DataFrame with one row per DIC sample.</p> <pre><code>logfile = ksv.read_logfile(\"path/to/logfile.bak\", methods=\"3C standard\")\n</code></pre> <p><code>read_logfile</code>: optional keyword arguments</p> <ul> <li><code>methods</code>: list of VINDTA method filenames used to run samples, excluding the <code>.mth</code> extensions</li> </ul>"},{"location":"vindta/#the-dbs-file","title":"The dbs file","text":"<p>Import a <code>.dbs</code> file into an enhanced DataFrame and rename its columns into a friendlier format.</p> <pre><code>dbs = ksv.read_dbs(\"path/to/file.dbs\", keep_all_cols=False)\n</code></pre> <p><code>read_dbs</code>: optional keyword arguments</p> <ul> <li><code>keep_all_cols</code>: retain all columns from the <code>.dbs</code> (<code>True</code>) or just the most important ones (<code>False</code>)?</li> </ul>"},{"location":"vindta/#add-sample-metadata","title":"Add sample metadata","text":"<p>Once you've imported the files above, you need to add the following metadata as extra columns in the <code>dbs</code> DataFrame under the following column labels:</p> <ul> <li><code>\"salinity\"</code>: practical salinity (assumed 35 if not provided)</li> <li><code>\"temperature_analysis_dic\"</code>: temperature of DIC analysis in \u00b0C (assumed 25 \u00b0C if not provided)</li> <li><code>\"dic_certified\"</code>: certified DIC values for reference materials in \u03bcmol/kg-sw.  Non-reference samples should be set to <code>np.nan</code>.</li> </ul> <p>You can also add the following logical columns to refine which samples and reference materials are used for processing and calibration:</p> <ul> <li><code>\"blank_good\"</code>: use this sample in assessing the coulometer blank value?</li> <li><code>\"k_dic_good\"</code>: use this reference material for calibration?</li> </ul> <p>As a starting point, you could set all <code>\"blank_good\"</code> values to <code>True</code> and <code>\"k_dic_good\"</code> to <code>True</code> for all CRMs but <code>False</code> everywhere else.</p> <p>A complete example script up to this point might look something like this:</p> <pre><code>import numpy as np\nfrom koolstof import vindta as ksv\n\n# Import files from VINDTA\nlogfile = ksv.read_logfile(\"path/to/logfile.bak\")\ndbs = ksv.read_dbs(\"path/to/dbsfile.dbs\")\n\n# Assign certified DIC for CRMs\n# - In this example, we assume that the 'bottle' column of the dbs always\n#   starts with the letters \"CRM\" for CRMs, and that all CRMs have a\n#   certified value of 2017.45 \u00b5mol/kg\ndbs[\"dic_certified\"] = np.where(\n    dbs.bottle.str.startswith(\"CRM\"), 2017.45, np.nan\n)\n\n# Assign analysis temperature, if it's not 25 \u00b0C\ndbs[\"temperature_analysis_dic\"] = 23.0\n\n# Assign dbs[\"salinity\"] here too, if it's not always 35\n\n# Set up logicals\ndbs[\"blank_good\"] = True\ndbs[\"k_dic_good\"] = ~dbs.dic_certified.isnull()\n</code></pre> <p>Now we're ready to apply the blank corrections and then calibrate.</p>"},{"location":"vindta/#find-and-apply-blank-corrections","title":"Find and apply blank corrections","text":"<p>To find and apply the blank corrections, use <code>ksv.blank_correction()</code>.</p> <pre><code>sessions = ksv.blank_correction(\n    dbs,\n    logfile,\n    blank_col=\"blank\",\n    counts_col=\"counts\",\n    runtime_col=\"run_time\",\n    session_col=\"dic_cell_id\",\n    use_from=6,\n)\n</code></pre> <p><code>blank_correction</code>: optional keyword arguments</p> <ul> <li><code>blank_col</code>: the name of the column containing the blank for each sample.</li> <li><code>counts_col</code>: the name of the column containing the raw counts for each sample.</li> <li><code>runtime_col</code>: the name of the column containing the total run time for each sample.</li> <li><code>session_col</code>: the name of the column containing the analysis session identifiers.</li> <li><code>use_from</code>: which minute of the coulometric titrations to measure the blank starting from.</li> </ul> <p>The output <code>sessions</code> is a table of analysis sessions, as identified by unique values of the <code>session_col</code>.  The <code>dbs</code> is also updated with extra columns, most importantly, <code>\"blank_here\"</code> and <code>\"counts_corrected\"</code>.</p> <p>After running <code>blank_correction</code>, you should make a few plots to check that everything has worked as intended.</p>"},{"location":"vindta/#plot-the-count-increments","title":"Plot the count increments","text":"<p>First, check that you are using an appropriate value of <code>use_from</code>.  In general, in a coulometric titration, all the CO<sub>2</sub> from the sample passes through the coulometer within the first few minutes, so the last few minutes can be considered as 'blank' measurements.  koolstof determines the blank first on a sample-by-sample basis starting from whatever minute is specified by <code>use_from</code>, so we should check that the increments from this point actually are constant.  To do this:</p> <pre><code>ksv.plot_increments(dbs, logfile, use_from=6)\n</code></pre> <p>This generates a figure like below:</p> <p></p> <p>Here we see the count increments for every sample in the dbs.  The y-axis is automatically zoomed in on the lower values at the end of the analysis, which we use to determine the blank.  The data points currently considered as 'blanks', where the number of minutes is greater than or equal to <code>use_from</code>, are shown in red.  We need to check that all the sample has indeed passed through this point, and that there isn't a strong trend with time in the red points.</p> <p>In the example above, the points at minute 6 (and possibly 7) are still a bit higher than the points later on, so it would probably be better to switch to <code>use_from=7</code> (or <code>use_from=8</code>) when running <code>ksv.blank_correction</code> on this dataset.</p>"},{"location":"vindta/#plot-the-session-blank-fits","title":"Plot the session blank fits","text":"<p>Once we're happy with the <code>use_from</code> value, we can plot the blank fits on a session-by-session basis.  Although we determine an individual blank value for each sample, koolstof doesn't use these directly to make the blank correction.  The results are better if you fit a curve through the blank values for each analysis session.  To visualise the fitted curves for all the analysis sessions in your dbs, use:</p> <pre><code>ksv.plot_blanks(dbs, sessions)\n</code></pre> <p>This generates a sequence of plots, each something like this:</p> <p></p> <p>The points are the sample-by-sample blank values, with error bars indicating the standard deviation of the minute-by-minute blank estimates for each sample.  The solid line shows the fit, which is what's actually used to make the blank correction for each sample.</p> <p>If any points fall far away from the rest and are causing the solid line to not have a good fit to the data, you can ignore them (in this curve fitting step only) by setting the <code>\"blank_good\"</code> column of <code>dbs</code> to <code>False</code> for those rows.  Then, re-run <code>ksv.blank_correction</code>.  These points will subsequently show up as open symbols on these plots (see legend - 'Ignored') and won't influence the fitted line.</p> <p>A DIC result will still be returned for these points, so you should check whether it looks sensible, as an unusual blank value may indicate that something may have gone wrong with that particular analysis.</p>"},{"location":"vindta/#calibrate-dic-measurements","title":"Calibrate DIC measurements","text":"<p>Once the blank correction is complete, you can calibrate the DIC measurements based on CRMs.</p> <pre><code>ksv.calibrate_dic(dbs, sessions)\n</code></pre> <p>This adds extra columns to <code>dbs</code> and <code>sessions</code> with various calibration metadata.  The final DIC result can be found in <code>dbs.dic</code>.</p> <p>Next, you should visualise the calibration factors and exclude any bad CRM measurements from the calibration.</p>"},{"location":"vindta/#plot-calibration-factors","title":"Plot calibration factors","text":"<p>To see all the calibration factors in the dbs through time, use:</p> <pre><code>ksv.plot_k_dic(dbs, sessions, show_ignored=True)\n</code></pre> <p>The plot may look something like this:</p> <p></p> <p>Each change of colour and marker style indicates a new analysis session, and the horizontal lines show the mean calibration factors used for each session.</p> <p>In the example above, there are clearly two bad CRM measurements.  You can exclude these from the calibration by setting <code>\"k_dic_good\"</code> to false, then re-run <code>ksv.calibrate_dic</code> to recalibrate.  Recreating the figure above but with <code>show_ignored=False</code> now gives us a clearer picture of the calibrations:</p> <p></p>"},{"location":"vindta/#plot-crm-offsets","title":"Plot CRM offsets","text":"<p>Finally, we can visualise the same information above in a different way by looking at the offsets between each CRM after calibration and its certified value:</p> <pre><code>ksv.plot_dic_offset(dbs, sessions)\n</code></pre> <p></p> <p>The CRMs for each analysis session will fall on average at zero.  The scatter about zero gives some indication of the precision of the measurement.</p> <p>In this example, we might also consider checking the lab notebook to see if there are any reasons why the lower purple point in the first session could be excluded from the calibration.</p>"},{"location":"vindta/#summary","title":"Summary","text":"<p>A complete example of your calibration code might look something like this (the first part is copied from the example higher up the page):</p> <pre><code>import numpy as np\nfrom koolstof import vindta as ksv\n\n# Import files from VINDTA\nlogfile = ksv.read_logfile(\"path/to/logfile.bak\")\ndbs = ksv.read_dbs(\"path/to/dbsfile.dbs\")\n\n# Assign certified DIC for CRMs\n# - In this example, we assume that the 'bottle' column of the dbs always\n#   starts with the letters \"CRM\" for CRMs, and that all CRMs have a\n#   certified value of 2017.45 \u00b5mol/kg\ndbs[\"dic_certified\"] = np.where(\n    dbs.bottle.str.startswith(\"CRM\"), 2017.45, np.nan\n)\n\n# Assign analysis temperature, if it's not 25 \u00b0C\ndbs[\"temperature_analysis_dic\"] = 23.0\n\n# Assign dbs[\"salinity\"] here too, if it's not always 35\n\n# Set up logicals\ndbs[\"blank_good\"] = True\ndbs[\"k_dic_good\"] = ~dbs.dic_certified.isnull()\n\n# Here, set any values of dbs.blank_good to False as needed from looking\n# at the figures below \n\n# Find and apply blank corrections\nsessions = ksv.blank_correction(dbs, use_from=8)\n\n# Visualise blank corrections\nksv.plot_increments(dbs, logfile)\nksv.plot_blanks(dbs, sessions)\n\n# Here, set any values of dbs.k_dic_good to False as needed from looking\n# at the figures below\n\n# Calibrate DIC\nksv.calibrate_dic(dbs, sessions)\n\n# Visualise the calibration\nksv.plot_k_dic(dbs, sessions, show_ignored=False)\nksv.plot_dic_offset(dbs, sessions)\n</code></pre>"}]}